# -*- coding: utf-8 -*-
"""Infosys 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fTi99zsaluVZxGLrZl33Q-jxduYTifVq

# **`Data Collection and Input Handling`**:
"""

# Run this first to preview the raw file structure
with open('bbc-news-data.csv', 'r', encoding='utf-8', errors='replace') as f:
    for i, line in enumerate(f):
        print(i+1, repr(line[:400]))   # prints first 400 chars of each line
        if i == 9: break

import pandas as pd

try:
    df = pd.read_csv('bbc-news-data.csv', sep='\t', engine='python', encoding='utf-8')
    print("Loaded with sep='\\t'. Shape:", df.shape)
    display(df.head())
except Exception as e:
    print("Failed with sep='\\t':", e)

rows, cols = df.shape
print("Total rows:", rows)
print("Total columns:", cols)

"""
# **`DATA PREPROCESSING`**:

"""

df.isnull().sum()

import re

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

df['clean_content'] = df['content'].apply(clean_text)
df.head()

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

stop = set(stopwords.words('english'))

df['clean_content'] = df['clean_content'].apply(
    lambda x: " ".join([w for w in x.split() if w not in stop])
)

from nltk.stem import PorterStemmer
ps = PorterStemmer()

df["stemmed"] = df["clean_content"].apply(
    lambda x: " ".join(ps.stem(word) for word in x.split())
)

df['stemmed'].head()

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
lem = WordNetLemmatizer()

df["lemmatized"] = df["clean_content"].apply(
    lambda x: " ".join(lem.lemmatize(word) for word in x.split())
)

df['lemmatized'].head()

"""# **`TOPIC MODELING IMPLEMENTATION`**:

# **LDA - Latent Dirichlet Allocation**
"""

# Step 1: Tokenization + Vectorization
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(
    stop_words='english',  # remove common words
    max_df=0.9,            # ignore very frequent words
    min_df=5               # ignore very rare words
)

dtm = vectorizer.fit_transform(df['clean_content'])

print("Document-Term Matrix shape:", dtm.shape)

!pip install gensim

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from gensim.models.coherencemodel import CoherenceModel
from gensim.corpora.dictionary import Dictionary
import matplotlib.pyplot as plt

# TEXTS = TOKENIZED DOCUMENTS
texts = [doc.split() for doc in df["clean_content"]]

# Dictionary + corpus
dictionary = Dictionary(texts)
corpus = [dictionary.doc2bow(t) for t in texts]

# DTM
vectorizer = CountVectorizer(stop_words='english', max_df=0.9, min_df=7)
dtm = vectorizer.fit_transform(df['clean_content'])
feature_names = vectorizer.get_feature_names_out()

# Function to get top words
def get_topic_words(model, num_words=10):
    topic_words = []
    for comp in model.components_:
        words = [feature_names[i] for i in comp.argsort()[:-num_words-1:-1]]
        topic_words.append(words)
    return topic_words

topics_range = range(1, 10)
perplexities = []
coherences = []

for n in topics_range:
    print("\n" + "="*60)
    print(f"üìå LDA with {n} Topics")
    print("="*60)

    lda = LatentDirichletAllocation(n_components=n, random_state=42)
    lda.fit(dtm)

    # Perplexity
    perp = lda.perplexity(dtm)
    perplexities.append(perp)

    # Topic words
    topics = get_topic_words(lda)

    # Coherence
    cm = CoherenceModel(
        topics=topics, texts=texts,
        dictionary=dictionary, coherence='c_v'
    )
    coherence = cm.get_coherence()
    coherences.append(coherence)

    print(f"Perplexity: {perp:.3f}")
    print(f"Coherence: {coherence:.3f}")
    print("\nüîπ Topic Word Distributions")

    # Display each topic with words
    for idx, words in enumerate(topics):
        print(f"  Topic {idx+1}: {', '.join(words)}")

# # Optional Plot
# plt.figure(figsize=(10,5))
# plt.plot(topics_range, perplexities, marker='o', label="Perplexity")
# plt.plot(topics_range, coherences, marker='s', label="Coherence (C_v)")
# plt.xlabel("Number of Topics")
# plt.ylabel("Score")
# plt.title("LDA: Perplexity vs Coherence")
# plt.legend()
# plt.grid(True)
# plt.show()

import matplotlib.pyplot as plt

fig, ax1 = plt.subplots(figsize=(10,5))

# LEFT AXIS ‚Üí Perplexity
ax1.plot(topics_range, perplexities, marker='o', color='blue', label='Perplexity')
ax1.set_xlabel("Number of Topics")
ax1.set_ylabel("Perplexity", color='blue')
ax1.tick_params(axis='y', labelcolor='blue')

# RIGHT AXIS ‚Üí Coherence
ax2 = ax1.twinx()
ax2.plot(topics_range, coherences, marker='s', color='orange', label='Coherence (C_v)')
ax2.set_ylabel("Coherence Score", color='orange')
ax2.tick_params(axis='y', labelcolor='orange')

plt.title("LDA Evaluation: Perplexity vs Coherence (Dual Axis)")
fig.tight_layout()
plt.grid(True)
plt.show()

# Train final LDA with 6 topics
final_lda = LatentDirichletAllocation(
    n_components=6,
    random_state=42
)

final_lda.fit(dtm)

print("Final LDA Model Trained with 6 Topics")

topic_labels = {
    0: "Technology & Digital Media",
    1: "Film / Sports / Entertainment",
    2: "Business & Companies",
    3: "Economy & Market Growth",
    4: "Government & Public Affairs",
    5: "Political Leaders / Elections"
}

for i, label in topic_labels.items():
    print(f"Topic {i+1} ‚Üí {label}")

doc_topic_matrix = final_lda.transform(dtm)

import numpy as np

dominant_topic = np.argmax(doc_topic_matrix, axis=1)
dominant_topic[:20]  # show the first 20 documents

# Get topic probabilities for each document
doc_topic_matrix = final_lda.transform(dtm)

# Convert to DataFrame
topic_df = pd.DataFrame(
    doc_topic_matrix,
    columns=[f"Topic_{i+1}" for i in range(6)]
)

# Add document index
topic_df.insert(0, "Document", topic_df.index)

# Show first 10 documents
topic_df.head(10)

"""# **NMF - Non-Negative Matrix Factorization**"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(
    stop_words='english',
    max_df=0.9,
    min_df=7
)

tfidf = tfidf_vectorizer.fit_transform(df["clean_content"])
feature_names = tfidf_vectorizer.get_feature_names_out()
from sklearn.decomposition import NMF
import numpy as np

topics_range = range(1, 10)
nmf_reconstruction_errors = []

for n in topics_range:
    nmf_model = NMF(n_components=n, random_state=42)
    nmf_model.fit(tfidf)

    nmf_reconstruction_errors.append(nmf_model.reconstruction_err_)

import matplotlib.pyplot as plt

plt.figure(figsize=(8,4))
plt.plot(topics_range, nmf_reconstruction_errors, marker='o')
plt.title("NMF: Reconstruction Error vs Number of Topics")
plt.xlabel("Number of Topics")
plt.ylabel("Reconstruction Error")
plt.grid(True)
plt.show()

print("==== Step 1: Creating TF-IDF Vectorizer & Matrix ====")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import numpy as np
import joblib
import os

# Ensure models folder exists
os.makedirs("models", exist_ok=True)

# ---- TF-IDF VECTORISER (MODEL, NOT MATRIX) ----
tfidf_vectorizer = TfidfVectorizer(
    stop_words="english",
    max_df=0.9,
    min_df=7
)

tfidf_matrix = tfidf_vectorizer.fit_transform(df["clean_content"])
feature_names = tfidf_vectorizer.get_feature_names_out()

print("TF-IDF Matrix Shape:", tfidf_matrix.shape)
print("Number of Features:", len(feature_names))
print("\n")


print("==== Step 2: Running NMF for 1 to 9 Topics ====")

topics_range = range(1, 10)
nmf_errors = []

best_n = None
best_error = float("inf")
best_nmf_model = None

for n in topics_range:
    print(f"Training NMF with {n} topics...")

    nmf_model = NMF(
        n_components=n,
        random_state=42,
        init="nndsvd"
    )
    nmf_model.fit(tfidf_matrix)

    error = nmf_model.reconstruction_err_
    nmf_errors.append(error)

    print(f"  Reconstruction Error: {error:.4f}")
    print("-" * 50)

    # Track best model
    if error < best_error:
        best_error = error
        best_n = n
        best_nmf_model = nmf_model

print("\nAll Reconstruction Errors:", nmf_errors)
print(f"\nBest number of topics: {best_n}")
print("\n")


print("==== Step 3: SAVING MODELS (IMPORTANT STEP) ====")

# üîë THIS IS THE CRITICAL FIX
joblib.dump(tfidf_vectorizer, "models/tfidf_vectorizer.pkl")
joblib.dump(best_nmf_model, "models/nmf_model.pkl")

print("‚úÖ Saved:")
print(" - models/tfidf_vectorizer.pkl (VECTORISER MODEL)")
print(" - models/nmf_model.pkl (NMF MODEL)")

print("==== Step 3: Plotting Reconstruction Error Curve ====")

import matplotlib.pyplot as plt

plt.figure(figsize=(10,5))
plt.plot(topics_range, nmf_errors, marker='o')
plt.title("NMF: Reconstruction Error vs Number of Topics")
plt.xlabel("Number of Topics")
plt.ylabel("Reconstruction Error")
plt.grid(True)
plt.show()

def get_nmf_topic_words(model, num_words=15):
    topic_words = []
    for idx, topic in enumerate(model.components_):
        top_indices = topic.argsort()[:-num_words-1:-1]
        words = [feature_names[i] for i in top_indices]
        topic_words.append(words)
    return topic_words

from gensim.models import CoherenceModel
from gensim.corpora.dictionary import Dictionary

# Use your existing tokenized texts
dictionary = Dictionary(texts)

nmf_coherences = []

print("==== Calculating NMF Coherence Scores ====")

for n in range(1, 10):
    print(f"\nTraining NMF model with {n} topics...")

    nmf_model = NMF(n_components=n, random_state=42, init='nndsvd')
    nmf_model.fit(tfidf)

    # Get top words for this model
    nmf_topics = []
    for topic in nmf_model.components_:
        top_indices = topic.argsort()[:-15-1:-1]
        nmf_topics.append([feature_names[i] for i in top_indices])

    # Compute Coherence
    cm = CoherenceModel(
        topics=nmf_topics,
        texts=texts,
        dictionary=dictionary,
        coherence='c_v'
    )

    coherence_val = cm.get_coherence()
    nmf_coherences.append(coherence_val)

    print(f"Coherence Score for {n} topics: {coherence_val:.4f}")

plt.figure(figsize=(10,5))
plt.plot(range(1, 10), nmf_coherences, marker='s', color='purple')
plt.title("NMF: Coherence Score vs Number of Topics")
plt.xlabel("Number of Topics")
plt.ylabel("Coherence Score (C_v)")
plt.grid(True)
plt.show()

print("==== Step 4: Training Final NMF Model (5 Topics) ====")


nmf_final = NMF(n_components=5, random_state=42, init='nndsvd')
nmf_final.fit(tfidf)

print("Final NMF Model Trained with 5 Topics\n")


print("==== Step 5: Extracting Top Words for NMF Topics ====")

def get_nmf_topic_words(model, num_words=15):
    topic_words = []
    for idx, topic in enumerate(model.components_):
        print(f"\nTopic {idx+1} Words:")
        top_indices = topic.argsort()[:-num_words-1:-1]
        words = [feature_names[i] for i in top_indices]
        topic_words.append(words)
        print(", ".join(words))
    return topic_words

nmf_topics = get_nmf_topic_words(nmf_final)
print("\nDone.")

final_topics = get_topic_words(final_lda, num_words=15)

for i, topic in enumerate(final_topics):
    print(f"\nTopic {i+1}: {', '.join(topic)}")

lda_coherence_scores = coherences        # from your LDA loop
nmf_coherence_scores = nmf_coherences    # from your NMF loop

topics_range = range(1, 10)

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(10,5))
plt.plot(topics_range, lda_coherence_scores, marker='o', label='LDA Coherence')
plt.plot(topics_range, nmf_coherence_scores, marker='s', label='NMF Coherence')

plt.title("LDA vs NMF: Coherence Score Comparison")
plt.xlabel("Number of Topics")
plt.ylabel("Coherence Score (C_v)")

# Set clean y-axis ticks
plt.yticks(np.arange(0.3, 0.85, 0.05))
# Shows ticks at: 0.30, 0.35, 0.40, ... , 0.80

plt.legend()
plt.grid(True)
plt.show()

nmf_doc_topic = nmf_final.transform(tfidf)

df["nmf_dominant_topic"] = np.argmax(nmf_doc_topic, axis=1)
df["nmf_topic_confidence"] = nmf_doc_topic.max(axis=1)

nmf_topic_counts = np.bincount(df["nmf_dominant_topic"])

plt.figure(figsize=(8,5))
plt.bar(range(1, 6), nmf_topic_counts)
plt.xlabel("Topic Number")
plt.ylabel("Number of Documents")
plt.title("Document Distribution Across Final 5 NMF Topics")
plt.show()

"""# **`SENTIMENT ANALYSIS`**:"""

!pip install nltk

import nltk
nltk.download('vader_lexicon')

from nltk.sentiment.vader import SentimentIntensityAnalyzer

sia = SentimentIntensityAnalyzer()

# Apply VADER to each document
df["sentiment_scores"] = df["clean_content"].apply(lambda x: sia.polarity_scores(x))

df["sentiment_scores"]

df["compound"] = df["sentiment_scores"].apply(lambda x: x["compound"])

def get_sentiment_label(score):
    if score >= 0.05:
        return "Positive"
    elif score <= -0.05:
        return "Negative"
    else:
        return "Neutral"

df["sentiment_label"] = df["compound"].apply(get_sentiment_label)
df["dominant_topic"] = dominant_topic
df["topic_confidence"] = doc_topic_matrix.max(axis=1)
topic_labels = {
    0: "Technology & Digital Media",
    1: "Film / Sports / Entertainment",
    2: "Business & Companies",
    3: "Economy & Market Growth",
    4: "Government & Public Affairs",
    5: "Political Leaders / Elections"
}

df["topic_name"] = df["dominant_topic"].map(topic_labels)
topic_sentiment_summary = df.groupby(["topic_name", "sentiment_label"]).size().unstack(fill_value=0)
topic_sentiment_summary
import matplotlib.pyplot as plt

df["sentiment_label"].value_counts().plot(kind='bar', figsize=(6,4))
plt.title("Overall Sentiment Distribution")
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.show()

# Neutral sentiment was negligible and therefore merged into Negative for clearer interpretation.
df["compound"] = df["sentiment_scores"].apply(lambda x: x["compound"])

def get_sentiment_label(score):
    if score >= 0.05:
        return "Positive"
    else:
        return "Negative"

df["sentiment_label"] = df["compound"].apply(get_sentiment_label)

# df["dominant_topic"] = dominant_topic
# df["topic_confidence"] = doc_topic_matrix.max(axis=1)
# topic_labels = {
#     0: "Technology & Digital Media",
#     1: "Film / Sports / Entertainment",
#     2: "Business & Companies",
#     3: "Economy & Market Growth",
#     4: "Government & Public Affairs",
#     5: "Political Leaders / Elections"
# }

# df["topic_name"] = df["dominant_topic"].map(topic_labels)
# topic_sentiment_summary = df.groupby(["topic_name", "sentiment_label"]).size().unstack(fill_value=0)
# topic_sentiment_summary

nmf_doc_topic = nmf_final.transform(tfidf)
df["nmf_dominant_topic"] = np.argmax(nmf_doc_topic, axis=1)
df["nmf_topic_confidence"] = nmf_doc_topic.max(axis=1)
nmf_topic_labels = {
    0: "Sports",
    1: "Politics & Government",
    2: "Technology & Digital Media",
    3: "Movies & Entertainment",
    4: "Economy & Business"
}
df["nmf_topic_name"] = df["nmf_dominant_topic"].map(nmf_topic_labels)
topic_sentiment_summary = (
    df.groupby(["nmf_topic_name", "sentiment_label"])
      .size()
      .unstack(fill_value=0)
)

topic_sentiment_summary

import matplotlib.pyplot as plt

df["sentiment_label"].value_counts().plot(kind='bar', figsize=(6,4))
plt.title("Overall Sentiment Distribution")
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.show()

topic_sentiment_summary.plot(kind='bar', figsize=(10,6))
plt.title("Sentiment Distribution Across Topics")
plt.xlabel("Topic")
plt.ylabel("Document Count")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(6,4))
plt.hist(df["nmf_topic_confidence"], bins=20)
plt.title("Distribution of Topic Confidence (NMF)")
plt.xlabel("Confidence Score")
plt.ylabel("Frequency")
plt.show()

final_output = df[[
    "content",
    "clean_content",
    "nmf_dominant_topic",
    "nmf_topic_name",
    "nmf_topic_confidence",
    "compound",
    "sentiment_label"
]]

final_output.head(10)

def analyze_new_text(text):
    # 1Ô∏è‚É£ Clean text (same preprocessing)
    text_clean = clean_text(text)

    # 2Ô∏è‚É£ TF-IDF transform (DO NOT fit again)
    text_tfidf = tfidf_vectorizer.transform([text_clean])

    # 3Ô∏è‚É£ NMF topic prediction
    nmf_topic_dist = nmf_final.transform(text_tfidf)
    dominant_topic = np.argmax(nmf_topic_dist)
    topic_confidence = nmf_topic_dist.max()
    topic_name = nmf_topic_labels[dominant_topic]

    # 4Ô∏è‚É£ Sentiment analysis (VADER)
    sentiment_scores = sia.polarity_scores(text_clean)
    compound_score = sentiment_scores["compound"]
    sentiment_label = get_sentiment_label(compound_score)

    # 5Ô∏è‚É£ Final result
    result = {
        "Original Text": text,
        "Cleaned Text": text_clean,
        "Predicted Topic": topic_name,
        "Topic Confidence": round(topic_confidence, 4),
        "Sentiment": sentiment_label,
        "Compound Score": round(compound_score, 4)
    }

    return result
new_text = "The government announced new economic reforms to boost market growth."

output = analyze_new_text(new_text)

for k, v in output.items():
    print(f"{k}: {v}")

new_texts = [
    "The company suffered heavy losses this quarter.",
    "The football team won the championship.",
    "New AI technology is transforming digital media."
]

for text in new_texts:
    print("\n---")
    result = analyze_new_text(text)
    for k, v in result.items():
        print(f"{k}: {v}")

"""# **`INSIGHTS GENERATION and SUMMARISATION`**:"""

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

from nltk.tokenize import sent_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
# Group original content by topic
topic_docs = df.groupby("topic_name")["content"].apply(list)

def extractive_summary(texts, num_sentences=3):
    # Combine all documents of a topic
    full_text = " ".join(texts)

    # Split into sentences
    sentences = sent_tokenize(full_text)

    if len(sentences) <= num_sentences:
        return " ".join(sentences)

    # TF-IDF on sentences
    vectorizer = TfidfVectorizer(stop_words="english")
    tfidf_matrix = vectorizer.fit_transform(sentences)

    # Score sentences
    sentence_scores = tfidf_matrix.sum(axis=1).A1

    # Select top sentences
    top_indices = np.argsort(sentence_scores)[-num_sentences:]
    top_indices.sort()

    summary = " ".join([sentences[i] for i in top_indices])
    return summary

# ‚úÖ Correct: group by NMF topic labels
topic_docs = df.groupby("nmf_topic_name")["content"].apply(list)
extractive_summaries = {}

for topic, docs in topic_docs.items():
    extractive_summaries[topic] = extractive_summary(docs, num_sentences=3)
for topic, summary in extractive_summaries.items():
    print("\nüîπ Topic:", topic)
    print("Extractive Summary:")
    print(summary)

# ‚úÖ Correct: group by NMF topic labels
topic_docs = df.groupby("nmf_topic_name")["content"].apply(list)
extractive_summaries = {}

for topic, docs in topic_docs.items():
    extractive_summaries[topic] = extractive_summary(docs, num_sentences=3)
for topic, summary in extractive_summaries.items():
    print("\nüîπ Topic:", topic)
    print("Extractive Summary:")
    print(summary)

!pip install transformers torch
from transformers import pipeline
summarizer = pipeline(
    "summarization",
    model="facebook/bart-large-cnn"
)

def abstractive_summary(texts, max_length=120):
    full_text = " ".join(texts)

    # Limit input size (important!)
    full_text = full_text[:3000]

    summary = summarizer(
        full_text,
        max_length=max_length,
        min_length=40,
        do_sample=False
    )

    return summary[0]["summary_text"]
abstractive_summaries = {}

for topic, docs in topic_docs.items():
    abstractive_summaries[topic] = abstractive_summary(docs)

for topic in extractive_summaries.keys():
    print("\n==============================")
    print("TOPIC:", topic)

    print("\nExtractive Summary:")
    print(extractive_summaries[topic])

    print("\nAbstractive Summary:")
    print(abstractive_summaries[topic])

import pandas as pd

comparison_df = pd.DataFrame({
    "Aspect": [
        "Readability",
        "Faithfulness",
        "Coverage",
        "Explainability",
        "Computational Cost"
    ],
    "Extractive Summarization": [
        "Moderate",
        "High",
        "High",
        "High",
        "Low"
    ],
    "Abstractive Summarization": [
        "High",
        "Medium",
        "High",
        "Low",
        "High"
    ]
})

comparison_df

"""# **`VISUALIZATION`**:"""

!pip install wordcloud
# from wordcloud import WordCloud
# import matplotlib.pyplot as plt
# for topic, docs in topic_docs.items():
#     text = " ".join(docs)

#     wordcloud = WordCloud(
#         width=800,
#         height=400,
#         background_color='white',
#         stopwords=set(stopwords.words('english'))
#     ).generate(text)

#     plt.figure(figsize=(10,5))
#     plt.imshow(wordcloud, interpolation='bilinear')
#     plt.axis("off")
#     plt.title(f"Word Cloud for Topic: {topic}")
#     plt.show()
from wordcloud import WordCloud
import matplotlib.pyplot as plt

def plot_nmf_wordcloud(model, feature_names, topic_idx, topic_label):
    # Get topic-word weights
    topic_weights = model.components_[topic_idx]

    # Create word:weight dictionary (top 40 words)
    word_freq = {
        feature_names[i]: topic_weights[i]
        for i in topic_weights.argsort()[:-41:-1]
    }

    # Generate word cloud
    wc = WordCloud(
        width=900,
        height=450,
        background_color="white"
    ).generate_from_frequencies(word_freq)

    # Plot
    plt.figure(figsize=(10,5))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title(f"Word Cloud ‚Äì {topic_label}", fontsize=14)
    plt.show()
for topic_idx, topic_label in nmf_topic_labels.items():
    plot_nmf_wordcloud(
        nmf_final,
        feature_names,
        topic_idx,
        topic_label
    )

from wordcloud import WordCloud
import matplotlib.pyplot as plt
import math

def plot_nmf_wordclouds_grid(model, feature_names, topic_labels, max_words=25):
    n_topics = len(topic_labels)
    n_cols = 2
    n_rows = math.ceil(n_topics / n_cols)

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 4 * n_rows))
    axes = axes.flatten()

    for idx, (topic_idx, topic_label) in enumerate(topic_labels.items()):
        topic_weights = model.components_[topic_idx]

        word_freq = {
            feature_names[i]: topic_weights[i]
            for i in topic_weights.argsort()[:-max_words-1:-1]
        }

        wc = WordCloud(
            width=600,
            height=300,
            background_color="white"
        ).generate_from_frequencies(word_freq)

        axes[idx].imshow(wc, interpolation="bilinear")
        axes[idx].axis("off")
        axes[idx].set_title(f"{topic_label}", fontsize=13)

    # Remove empty subplots if any
    for j in range(idx + 1, len(axes)):
        axes[j].axis("off")

    plt.tight_layout()
    plt.show()
plot_nmf_wordclouds_grid(
    nmf_final,
    feature_names,
    nmf_topic_labels,
    max_words=25
)

# Topic-wise Sentiment Distribution
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(7,5))
sns.countplot(
    data=df,
    x="nmf_topic_name",
    hue="sentiment_label"
)
plt.xticks(rotation=30)
plt.title("Sentiment Distribution Across Topics")
plt.xlabel("Topic")
plt.ylabel("Number of Articles")
plt.legend(title="Sentiment")
plt.tight_layout()
plt.show()

# Topic Confidence Distribution
plt.figure(figsize=(5,3))
sns.histplot(df["nmf_topic_confidence"], bins=20, kde=True)
plt.title("Distribution of NMF Topic Confidence Scores")
plt.xlabel("Confidence Score")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

sentiment_map = {
    "Positive": 1,
    "Neutral": 0,
    "Negative": -1
}

df["sentiment_score"] = df["sentiment_label"].map(sentiment_map)

summary_lengths = []

for topic in extractive_summaries.keys():
    summary_lengths.append({
        "Topic": topic,
        "Method": "Extractive",
        "Length": len(extractive_summaries[topic].split())
    })
    summary_lengths.append({
        "Topic": topic,
        "Method": "Abstractive",
        "Length": len(abstractive_summaries[topic].split())
    })

summary_df = pd.DataFrame(summary_lengths)

plt.figure(figsize=(10,5))
sns.barplot(
    data=summary_df,
    x="Topic",
    y="Length",
    hue="Method"
)
plt.xticks(rotation=30)
plt.title("Summary Length Comparison by Method")
plt.tight_layout()
plt.show()

#Topic‚ÄìKeyword Heatmap
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

# Top words per topic
top_n = 10
topic_word_matrix = []

for topic_idx, topic in enumerate(nmf_final.components_):
    top_indices = topic.argsort()[:-top_n-1:-1]
    for idx in top_indices:
        topic_word_matrix.append({
            "Topic": nmf_topic_labels[topic_idx],
            "Word": feature_names[idx],
            "Weight": topic[idx]
        })

heatmap_df = pd.DataFrame(topic_word_matrix)

pivot_df = heatmap_df.pivot(
    index="Word",
    columns="Topic",
    values="Weight"
).fillna(0)

plt.figure(figsize=(10,6))
sns.heatmap(pivot_df, cmap="YlGnBu")
plt.title("Topic‚ÄìKeyword Weight Heatmap (NMF)")
plt.show()

topic_counts = df["nmf_topic_name"].value_counts()

plt.figure(figsize=(8,5))
topic_counts.plot(kind="bar")
plt.xlabel("Topic")
plt.ylabel("Number of Documents")
plt.title("Document Distribution Across Topics")
plt.xticks(rotation=45)
plt.grid(axis="y")
plt.show()

"""**[SAVED MODEL](https://)**"""

import joblib

# Save models
joblib.dump(tfidf, "tfidf_vectorizer.pkl")
joblib.dump(nmf_final, "nmf_model.pkl")

print("Models saved successfully")

"""# **EXTRAS~**"""

rows, cols = df.shape
print("Total rows:", rows)
print("Total columns:", cols)

df[df['sentiment_label'] == "neutral"].shape[0]

df[df['sentiment_label'] == "positive"].shape[0]

df[df['sentiment_label'] == "negative"].shape[0]

# counts
counts = df['sentiment_label'].value_counts()
print(counts)

# proportions
print(df['sentiment_label'].value_counts(normalize=True))

from wordcloud import WordCloud

for i, topic in enumerate(final_topics):
    wc = WordCloud(width=600, height=400).generate(" ".join(topic))
    plt.figure(figsize=(6,4))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Topic {i+1}: {topic_labels[i]}")
    plt.show()

from sklearn.decomposition import LatentDirichletAllocation

lda_model = LatentDirichletAllocation(
    n_components=4,    # number of topics
    random_state=42
)

print("LDA model created")
lda_model.fit(dtm)
print("LDA model trained successfully")
def display_topics(model, feature_names, num_words=10):
    for topic_idx, topic in enumerate(model.components_):
        print(f"\nTopic {topic_idx + 1}:")
        print(" ".join(
            [feature_names[i] for i in topic.argsort()[:-num_words - 1:-1]]
        ))

feature_names = vectorizer.get_feature_names_out()
display_topics(lda_model, feature_names)
perplexity = lda_model.perplexity(dtm)
print("LDA Perplexity:", perplexity)

lda_model.fit(dtm)
print("LDA model trained successfully")

def display_topics(model, feature_names, num_words=10):
    for topic_idx, topic in enumerate(model.components_):
        print(f"\nTopic {topic_idx + 1}:")
        print(" ".join(
            [feature_names[i] for i in topic.argsort()[:-num_words - 1:-1]]
        ))

feature_names = vectorizer.get_feature_names_out()
display_topics(lda_model, feature_names)

perplexity = lda_model.perplexity(dtm)
print("LDA Perplexity:", perplexity)

from textblob import TextBlob

def get_sentiment(text):
    return TextBlob(text).sentiment.polarity

df['sentiment_score'] = df['clean_content'].apply(get_sentiment)

df['sentiment_label'] = df['sentiment_score'].apply(
    lambda x: "positive" if x > 0 else ("negative" if x < 0 else "neutral")
)

df[['title','sentiment_score','sentiment_label']].head()

df[['title','sentiment_score','sentiment_label']].head(100)

df[df['sentiment_label'] == "neutral"][['title', 'sentiment_score', 'sentiment_label']]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

vectorizer = CountVectorizer(max_df=0.9, min_df=5, stop_words='english')
dtm = vectorizer.fit_transform(df['clean_content'])

lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(dtm)

def show_topics(model, feature_names, n_top_words=10):
    for topic_idx, topic in enumerate(model.components_):
        print(f"Topic {topic_idx+1}:")
        print("  ".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))
        print()

show_topics(lda, vectorizer.get_feature_names_out())

import matplotlib.pyplot as plt
df['sentiment_label'].value_counts().plot(kind='bar')
plt.title('Sentiment label counts')
plt.show()

!pip install transformers sentencepiece

from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

df['summary'] = df['content'].apply(
    lambda x: summarizer(x, max_length=120, min_length=40, do_sample=False)[0]['summary_text']
)

df[['title','summary']].head()