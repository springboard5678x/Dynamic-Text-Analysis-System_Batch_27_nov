{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e76a0-87d6-40de-a562-e889b2b848ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT ALL NECESSARY REQUIREMENTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7514ebbb-c90c-4449-a459-37d7dcb5add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    r\"C:\\narative nexus\\Amazon_Reviews.csv\",\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54fd4d-c229-4397-b23d-d0a9bd76a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep required columns\n",
    "df = df[['Review Text', 'Rating','Review Title']].dropna()\n",
    "\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee9980-8afd-4bc0-a430-b76da171de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic data check\n",
    "#shape\n",
    "print(\"SHAPE:\\n\",df.shape)\n",
    "#info\n",
    "print(\"\\nINFO:\\n\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50270b93-817e-4d9e-b41f-3de050a5df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of DataFrame before dropping nulls: {df.shape}\")\n",
    "\n",
    "\n",
    "df_cleaned = df.dropna().copy()\n",
    "\n",
    "print(f\"Shape of DataFrame after dropping nulls: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5228ac9-a003-4606-be3e-34d18bcaac13",
   "metadata": {},
   "source": [
    "# **Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a72f74-1362-4efd-9dd0-1654b2684add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural Language Toolkit SETUP\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') # Added to resolve LookupError\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b687b2-281f-4f2c-b938-cde45d6d6208",
   "metadata": {},
   "source": [
    "# **Text Cleaning (Lowercasing, Special Character Removal, Stopword Removal)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a85e33-b1a6-45de-8a32-b8a076c1143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    if text is None: # Handle None values\n",
    "        return \"\"\n",
    "    # Remove URLs (http, https, www)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Remove hashtags and mentions\n",
    "    text = re.sub(r'#\\w+|@\\w+', '', text)\n",
    "\n",
    "    # Remove copyright symbols and special markers\n",
    "    text = re.sub(r'¬©|¬Æ|‚Ñ¢', '', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "    df['clean_text'] = df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159e999-7608-41bf-a6e0-418f9ada5676",
   "metadata": {},
   "source": [
    "# **Text Lemmatization and Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d03ea-333c-4c73-8506-5be475d7e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Get stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Process tokens: filter, lemmatize, and keep only alphabetic words\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        # Keep only alphabetic tokens with length > 2\n",
    "        if token.isalpha() and len(token) > 2:\n",
    "            if token not in stop_words:\n",
    "                lemmatized = lemmatizer.lemmatize(token)\n",
    "                processed_tokens.append(lemmatized)\n",
    "\n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b923566b-d4f6-4323-93a9-67c81468e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COL = 'Review Text'   # ‚Üê change ONLY this if needed\n",
    "\n",
    "df['cleaned_text'] = df[TEXT_COL].astype(str).apply(clean_text)\n",
    "df['tokens'] = df['cleaned_text'].apply(tokenize_and_lemmatize)\n",
    "df['lemmatized_text'] = df['tokens'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d8a81-0282-4b26-9b78-aa0facf916bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[TEXT_COL, 'cleaned_text', 'tokens', 'lemmatized_text']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d088762-a760-43b6-9e8f-35011a30fd8d",
   "metadata": {},
   "source": [
    "# **TOPIC MODELLING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00100c92-73e9-42c6-a07a-05bbae51b376",
   "metadata": {},
   "source": [
    "# **LATENT DIRICHLET ALLOCATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d558dd-75ed-487b-8bcc-9ed9abe843f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c143085-25f1-4c62-b560-a24b474ff111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import gensim\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d4a9be-923f-4bd3-9a71-d53e9f610019",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = df['tokens'].tolist()\n",
    "# CREATE DICTIONARY & CORPUS ---\n",
    "# Create Dictionary\n",
    "id2word = Dictionary(data_words)\n",
    "\n",
    "# Filter extremes: Remove words in <15 docs or >50% of docs\n",
    "id2word.filter_extremes(no_below=15, no_above=0.5)\n",
    "\n",
    "# Create Corpus (Term Document Frequency)\n",
    "corpus = [id2word.doc2bow(text) for text in data_words]\n",
    "\n",
    "print(f\"   - Dictionary size: {len(id2word)} unique tokens\")\n",
    "print(f\"   - Corpus size: {len(corpus)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968ab2b-223f-483e-b04c-0d6d68c45f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lda_models(dictionary, corpus, texts, start, limit, step):\n",
    "    coherence_values = []\n",
    "    perplexity_values = []\n",
    "    model_list = []\n",
    "\n",
    "    for num_topics in range(start, limit, step):\n",
    "        print(f\"Training model with {num_topics} topics...\")\n",
    "        # Build LDA model\n",
    "        model = gensim.models.ldamodel.LdaModel(\n",
    "            corpus=corpus,\n",
    "            num_topics=num_topics,\n",
    "            id2word=dictionary,\n",
    "            random_state=100,\n",
    "            passes=10,\n",
    "            alpha=0.01\n",
    "        )\n",
    "        model_list.append(model)\n",
    "\n",
    "        # Compute Coherence Score\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "        # Compute Perplexity score\n",
    "        log_perp = model.log_perplexity(corpus)\n",
    "        perplexity_values.append(np.exp(-1 * log_perp))\n",
    "\n",
    "    return model_list, coherence_values, perplexity_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bb1dc-f3e1-4fcd-83b4-83094f39d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start, limit, step = 5,30,5\n",
    "model_list, coherence_values, perplexity_values = evaluate_lda_models(\n",
    "    dictionary=id2word, corpus=corpus, texts=data_words, start=start, limit=limit, step=step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2981ee-c049-4c44-b3ea-fafcfafd8ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = list(range(start, limit + 1, step))\n",
    "\n",
    "print(\"Topic No | Coherence Score | Perplexity Score\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for k, coh, perp in zip(k_values, coherence_values, perplexity_values):\n",
    "    print(f\"{k:^8} | {coh:^15.4f} | {perp:^17.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644bb97-6d48-4c61-9441-73ea89564b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build k_values safely from coherence_values length\n",
    "k_values = list(range(start, start + step * len(coherence_values), step))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Coherence\n",
    "plt.plot(k_values,coherence_values,marker='o',label='Coherence Score')\n",
    "\n",
    "# Create second y-axis for Perplexity\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(k_values,perplexity_values,marker='s',linestyle='--',label='Perplexity Score')\n",
    "\n",
    "ax1.set_xlabel(\"Number of Topics (k)\")\n",
    "ax1.set_ylabel(\"Coherence Score\")\n",
    "ax2.set_ylabel(\"Perplexity Score\")\n",
    "\n",
    "plt.title(\"LDA Coherence & Perplexity vs Number of Topics\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='best')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1017f-c9c9-41c1-9567-3927a653ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = k_values[coherence_values.index(max(coherence_values))]\n",
    "best_coh = max(coherence_values)\n",
    "\n",
    "print(f\"Best Topic Count: {best_k}\")\n",
    "print(f\"Best Coherence Score: {best_coh:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f89122-ba56-471d-8dfb-7aaa1963cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model index using coherence\n",
    "best_index = coherence_values.index(max(coherence_values))\n",
    "\n",
    "# Compute best k\n",
    "best_k = start + best_index * step\n",
    "\n",
    "# Get best LDA model\n",
    "best_lda_model = model_list[best_index]\n",
    "\n",
    "print(f\"Best Topic Count: {best_k}\")\n",
    "print(f\"Best Coherence Score: {coherence_values[best_index]:.4f}\")\n",
    "\n",
    "# Display topics\n",
    "topics = best_lda_model.show_topics(\n",
    "    num_topics=best_k,\n",
    "    num_words=10,\n",
    "    formatted=False\n",
    ")\n",
    "\n",
    "for topic_id, words in topics:\n",
    "    print(f\"Topic {topic_id}: {[word for word, _ in words]}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca80bc7c-87e2-4ddf-89ea-2957d9ebc0d1",
   "metadata": {},
   "source": [
    "# **Non-Negative Matrix Factorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590f2c8-748c-4480-8bac-33ad1ca3ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ac0d9-9033-4179-9e35-a5ec659e9951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['lemmatized_text'])\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"TF-IDF shape:\", tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ec163-2f00-4452-b081-992d71ae2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "texts = df['tokens']\n",
    "\n",
    "dictionary = Dictionary(texts)\n",
    "dictionary.filter_extremes(\n",
    "    no_below=20,\n",
    "    no_above=0.4\n",
    ")\n",
    "\n",
    "print(\"Dictionary size:\", len(dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062842f-7cb4-4656-9f05-ab15a58d1fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nmf_coherence_values(tfidf,feature_names,texts,dictionary,start=5,limit=30,step=5):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, limit + 1, step):\n",
    "        nmf_model = NMF(n_components=num_topics,random_state=42,init='nndsvd',max_iter=500)\n",
    "        W = nmf_model.fit_transform(tfidf)\n",
    "        H = nmf_model.components_\n",
    "\n",
    "        model_list.append(nmf_model)\n",
    "\n",
    "        # Extract topics\n",
    "        topics = [\n",
    "            [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
    "            for topic in H\n",
    "        ]\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topics,texts=texts,dictionary=dictionary,coherence='c_v')\n",
    "\n",
    "        coherence = coherence_model.get_coherence()\n",
    "        coherence_values.append(coherence)\n",
    "\n",
    "        print(f\"Topics: {num_topics} | Coherence: {coherence:.4f}\")\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebee789-e89d-47e8-9ce8-9b7054178fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start, limit, step = 5, 25, 5\n",
    "\n",
    "nmf_models, nmf_coherence_values = compute_nmf_coherence_values(tfidf=tfidf,feature_names=feature_names,texts=df['tokens'],dictionary=dictionary,start=start,limit=limit,step=step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e942d6-9ca8-4ad1-84b6-0c531ab8339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "topic_range = list(range(start, limit + 1, step))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(topic_range, nmf_coherence_values, marker='o')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score (c_v)\")\n",
    "plt.title(\"NMF Topic Coherence\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c41d3a-df9f-4ee5-ad5a-ee65d8026f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_topics = topic_range[np.argmax(nmf_coherence_values)]\n",
    "print(\"Optimal number of topics:\", best_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8251b7-94b6-447c-9b70-40704e85d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "topic_range = list(range(start, limit + 1, step))\n",
    "\n",
    "best_index = np.argmax(nmf_coherence_values)\n",
    "optimal_topics = topic_range[best_index]\n",
    "best_nmf_model = nmf_models[best_index]\n",
    "\n",
    "print(f\"\\n Optimal number of topics: {optimal_topics}\\n\")\n",
    "\n",
    "# ----- DISPLAY NMF TOPICS -----\n",
    "n_top_words = 10\n",
    "\n",
    "for topic_idx, topic in enumerate(best_nmf_model.components_):\n",
    "    top_words = [\n",
    "        feature_names[i]\n",
    "        for i in topic.argsort()[:-n_top_words - 1:-1]\n",
    "    ]\n",
    "    print(f\"Topic {topic_idx}: {top_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa7910-8127-4d6b-9d0e-3c359fb906d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = {\n",
    "    0: \"Negative Reviews\",\n",
    "    1: \"Wrong Items\",\n",
    "    2: \"Customer Service\",\n",
    "    3: \"Positive Service\",\n",
    "    4: \"Shopping Experience\",\n",
    "    5: \"Prime Membership\",\n",
    "    6: \"Delivery Issues\",\n",
    "    7: \"Payment & Gift Cards\",\n",
    "    8: \"Positive Experience\",\n",
    "    9: \"Product Quality\",\n",
    "    10: \"Account Issues\",\n",
    "    11: \"Order Management\",\n",
    "    12: \"Delivery Speed\",\n",
    "    13: \"Online Shopping Satisfaction\",\n",
    "    14: \"Ease of Use\",\n",
    "    15: \"Shipping Delays\",\n",
    "    16: \"Fast Shipping\",\n",
    "    17: \"Time Wastage\",\n",
    "    18: \"Company Reputation\",\n",
    "    19: \"Refunds & Returns\"\n",
    "}\n",
    "table_data = []\n",
    "for topic_idx, topic in enumerate(best_nmf_model.components_):\n",
    "    top_words = [\n",
    "        feature_names[i]\n",
    "        for i in topic.argsort()[:-11:-1]\n",
    "    ]\n",
    "    table_data.append([\n",
    "        f\"Topic {topic_idx}\",\n",
    "        topic_labels.get(topic_idx, \"Unlabeled\"),\n",
    "        \", \".join(top_words)\n",
    "    ])\n",
    "\n",
    "row_count = len(table_data)\n",
    "fig_height = max(6, row_count * 0.45)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, fig_height))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=table_data,\n",
    "    colLabels=[\"Topic\", \"Label\", \"Top Keywords\"],\n",
    "    cellLoc=\"left\",\n",
    "    loc=\"center\"\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.auto_set_column_width(col=list(range(3)))\n",
    "table.scale(1, 1.2)\n",
    "\n",
    "plt.title(\"NMF Topics with Human-Readable Labels\", fontsize=16, pad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce9895f-41bc-4bd9-8fcd-05101289ab4e",
   "metadata": {},
   "source": [
    "# **COMPARASION BETWEEN THE COHERENCE SCORES OF LDA AND NMF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d00084-ac09-470c-8564-51506073cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "k_values_lda = list(range(5, 5 * (len(coherence_values) + 1), 5))\n",
    "k_values_nmf = list(range(5, 5 * (len(nmf_coherence_values) + 1), 5))\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "plt.plot(k_values_lda,coherence_values,marker='o',linestyle='-',label='LDA Coherence (c_v)')\n",
    "\n",
    "plt.plot(k_values_nmf,nmf_coherence_values,marker='s',linestyle='-',label='NMF Coherence (c_v)')\n",
    "\n",
    "plt.xlabel('Number of Topics (k)')\n",
    "plt.ylabel('Coherence Score (c_v)')\n",
    "plt.title('Comparison of LDA and NMF Coherence Scores')\n",
    "\n",
    "plt.xticks(sorted(set(k_values_lda + k_values_nmf)))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772ae6c-79ee-49bf-8118-c09ae480640d",
   "metadata": {},
   "source": [
    "# **SENTIMENTAL ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c7f75-35ce-4280-997d-0323cb36c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk vaderSentiment scikit-learn matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7eee46-412a-4899-978a-03c9aafac8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac14e7-82bb-4af8-8a8f-553552c1f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_sentiment(text):\n",
    "    score = analyzer.polarity_scores(text)[\"compound\"]\n",
    "    if score >= 0.1:\n",
    "        return \"Positive\"\n",
    "    elif score <= -0.1:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "df[\"Sentiment\"] = df[\"Review Text\"].apply(vader_sentiment)\n",
    "df[\"Sentiment\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8093964f-5dd6-4108-be46-abcbee66171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"Negative\": 0,\n",
    "    \"Neutral\": 1,\n",
    "    \"Positive\": 2\n",
    "}\n",
    "\n",
    "df[\"sentiment_label\"] = df[\"Sentiment\"].map(label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f14069-f6ca-4263-a247-bd88d7a89369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tfidf\n",
    "y = df[\"sentiment_label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9caf0cb-56f3-4182-966a-925509927e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "log_reg_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d961702f-c664-4e3a-926c-e103eeaeef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "y_pred = log_reg_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda9c13a-f69e-4910-955b-90862286e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Negative\", \"Neutral\", \"Positive\"],\n",
    "    yticklabels=[\"Negative\", \"Neutral\", \"Positive\"]\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Sentiment Classification Confusion Matrix (With Neutral)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0215ec7-a17b-48a8-a4d4-a15e55cdfc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integrate Sentiment with Topics\n",
    "nmf_doc_topics = best_nmf_model.transform(tfidf)\n",
    "df[\"dominant_topic\"] = np.argmax(nmf_doc_topics, axis=1)\n",
    "\n",
    "df[[TEXT_COL, \"dominant_topic\", \"Sentiment\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5f868-b3c5-4590-b198-ae93f49c03df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic-wise Sentiment Distribution\n",
    "topic_sentiment = pd.crosstab(df[\"dominant_topic\"], df[\"Sentiment\"])\n",
    "\n",
    "topic_sentiment.plot(kind=\"bar\", stacked=True, figsize=(10,5))\n",
    "plt.title(\"Topic-wise Sentiment Distribution\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efb1009-d665-48d6-97c5-4be47f9cddb6",
   "metadata": {},
   "source": [
    "#  **INSIGHT GENERATION & TEXT SUMMARIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91507f1-ac6a-40f2-9fb9-932f42403091",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = df[[\"Review Text\", \"dominant_topic\", \"Sentiment\"]].dropna()\n",
    "summary_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15234167-ca4e-4118-9d90-2b3663754dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "#Extractive Summarization Function\n",
    "def extractive_summary_short(texts, top_n=4, max_chars=50):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    scores = np.asarray(tfidf.sum(axis=1)).ravel()\n",
    "    top_indices = scores.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    summary = []\n",
    "    for i in top_indices:\n",
    "        line = texts[i]\n",
    "        line = line.replace(\"\\n\", \" \").strip()\n",
    "        if len(line) > max_chars:\n",
    "            line = line[:max_chars] + \" \"\n",
    "        summary.append(line)\n",
    "        \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96463b04-a6eb-43a7-ac48-00d892584919",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_summaries = {}\n",
    "\n",
    "for topic in sorted(summary_df[\"dominant_topic\"].unique()):\n",
    "    topic_texts = summary_df[\n",
    "        summary_df[\"dominant_topic\"] == topic\n",
    "    ][\"Review Text\"].tolist()\n",
    "    \n",
    "    if len(topic_texts) >= 4:\n",
    "        topic_summaries[topic] = extractive_summary_short(\n",
    "            topic_texts,\n",
    "            top_n=4,\n",
    "            max_chars=50\n",
    "        )\n",
    "    else:\n",
    "        topic_summaries[topic] = topic_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd8d507-8223-42c2-ace6-061f8ec7eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic, summaries in topic_summaries.items():\n",
    "    print(f\"\\nüîπ Topic {topic} Summary:\")\n",
    "    for s in summaries:\n",
    "        print(\"-\", s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c223ff5-e076-4638-a079-723a69e7889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment-wise Summarization\n",
    "sentiment_summaries = {}\n",
    "\n",
    "for sentiment in [\"Negative\", \"Neutral\", \"Positive\"]:\n",
    "    texts = summary_df[\n",
    "        summary_df[\"Sentiment\"] == sentiment\n",
    "    ][\"Review Text\"].tolist()\n",
    "    \n",
    "    if len(texts) >= 3:\n",
    "        sentiment_summaries[sentiment] = extractive_summary_short(texts, top_n=3)\n",
    "    else:\n",
    "        sentiment_summaries[sentiment] = texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef863d8-a68a-47ab-a190-e2c16a2ba390",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentiment, summaries in sentiment_summaries.items():\n",
    "    print(f\"\\nüî∏ {sentiment} Sentiment Summary:\")\n",
    "    for s in summaries:\n",
    "        print(\"-\", s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6498773c-5d89-43d3-affc-57d2c91081ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic + Sentiment Combined Insights\n",
    "for topic in sorted(summary_df[\"dominant_topic\"].unique()):\n",
    "    print(f\"\\nüìå Topic {topic} Insights:\")\n",
    "    \n",
    "    for sentiment in [\"Negative\", \"Neutral\", \"Positive\"]:\n",
    "        texts = summary_df[\n",
    "            (summary_df[\"dominant_topic\"] == topic) &\n",
    "            (summary_df[\"Sentiment\"] == sentiment)\n",
    "        ][\"Review Text\"].tolist()\n",
    "        \n",
    "        if len(texts) >= 2:\n",
    "            print(f\"\\n{sentiment} feedback:\")\n",
    "            for s in extractive_summary_short(texts, top_n=2):\n",
    "                print(\"-\", s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d22d42-b4d7-4e7b-9ad9-78773ac5d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"summarization_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"topic_summaries\": topic_summaries,\n",
    "        \"sentiment_summaries\": sentiment_summaries\n",
    "    }, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b64725e-a6b5-4445-8206-cf3f12a7e5c7",
   "metadata": {},
   "source": [
    "#  **VISUALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2aa5db-0b14-4f30-a886-cd64e84e2350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentiment_counts = df[\"Sentiment\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sentiment_counts.plot(kind=\"bar\")\n",
    "plt.title(\"Overall Sentiment Distribution\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb528786-bb35-41c4-90c9-d7fc7883e552",
   "metadata": {},
   "source": [
    "# Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8989c98d-afea-4b84-b061-60b820ac189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = df[\"dominant_topic\"].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "topic_counts.plot(kind=\"bar\")\n",
    "plt.title(\"Topic Distribution\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6626901d-1db7-4ef2-9a7c-0af2a10ddd4e",
   "metadata": {},
   "source": [
    "# Topic-wise Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758bdd82-22e8-4233-ab01-dcc89ba30d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "topic_sentiment = pd.crosstab(df[\"dominant_topic\"], df[\"Sentiment\"])\n",
    "\n",
    "topic_sentiment.plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    figsize=(10,5)\n",
    ")\n",
    "\n",
    "plt.title(\"Topic-wise Sentiment Distribution\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.legend(title=\"Sentiment\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29db9b3-690f-4e74-b020-3d66be425526",
   "metadata": {},
   "source": [
    "# WORLD CLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d6481-91c6-424c-bbc3-4ed732dce1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "word_weights = {}\n",
    "\n",
    "for topic in best_nmf_model.components_:\n",
    "    for word_idx, weight in enumerate(topic):\n",
    "        word = feature_names[word_idx]\n",
    "        word_weights[word] = word_weights.get(word, 0) + weight\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=1400,\n",
    "    height=700,\n",
    "    background_color=\"white\",\n",
    "    max_words=200,\n",
    "    collocations=False\n",
    ").generate_from_frequencies(word_weights)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\" Topics Word Cloud \", fontsize=18)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e2e62f-0065-4612-b04f-57d6d3d9509e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
